{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn 20newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn gensim nltk numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "TF-IDF Matrix Shape: (1000, 18204)\n",
      "Vocabulary: ['aa' 'aaa' 'aacc' 'aadnsi' 'aamir' 'aaph' 'aapjj' 'aapp' 'aaron'\n",
      " 'aausmausmaedu' 'aaverage' 'aaxclear' 'aaxdeoofsqfjmxhhls' 'ab'\n",
      " 'ababspalestinians' 'abandoned' 'abates' 'abberant' 'abbreviation' 'abc']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load dataset (using only first 1000 rows for efficiency)\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "texts = dataset.data[:1000]  \n",
    "\n",
    "# Preprocessing function\n",
    "stopwords_list = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization & Lowercasing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_list]  # Remove stopwords & lemmatize\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print results\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_array)\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_array.shape)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out()[:20])  # Print first 20 words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer': [-0.7163971   0.33736396  0.6031848  -0.02010767  0.20525196 -0.39639473\n",
      "  0.27783233  0.7710676   0.50691307  0.2177041 ]\n",
      "Words similar to 'computer': [('corporation', 0.410052627325058), ('tempest', 0.4002803564071655), ('desc', 0.3827262818813324), ('toshiba', 0.3660683035850525), ('kenneth', 0.36397117376327515), ('laptop', 0.35860878229141235), ('typewriter', 0.3546794652938843), ('bundled', 0.35299059748649597), ('dyer', 0.35050448775291443), ('math', 0.33884990215301514)]\n",
      "husband - man + woman ≈ [('aged', 0.36183658242225647), ('seeking', 0.35281890630722046), ('agdam', 0.3526656925678253), ('abusive', 0.3457013666629791), ('injured', 0.322365939617157), ('attacker', 0.3205150365829468), ('morgue', 0.3199331760406494), ('sasha', 0.31863734126091003), ('artillery', 0.31837305426597595), ('elderly', 0.3148309290409088)]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Prepare data for Word2Vec\n",
    "tokenized_texts = [text.split() for text in cleaned_texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         vector_size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "\n",
    "w2v_model.build_vocab(tokenized_texts, progress_per=10000)\n",
    "w2v_model.train(tokenized_texts, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "\n",
    "# Save the model \n",
    "w2v_model.save(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "# Test Word2Vec\n",
    "word = \"computer\"\n",
    "\n",
    "result = w2v_model.wv.most_similar(positive=['husband', 'woman'], negative=['man'])\n",
    "\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Vector for '{word}':\", w2v_model.wv[word][:10])  \n",
    "    print(f\"Words similar to '{word}':\", w2v_model.wv.most_similar(positive=[word]))\n",
    "    print(\"husband - man + woman ≈\", result)\n",
    "else:\n",
    "    print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents:\n",
      "Document 979 - Similarity Score: 0.9990\n",
      "Document 491 - Similarity Score: 0.9989\n",
      "Document 644 - Similarity Score: 0.9981\n",
      "Document 941 - Similarity Score: 0.9977\n",
      "Document 245 - Similarity Score: 0.9977\n",
      "Most relevant words to the first document: [('start', 0.9987062811851501), ('wont', 0.998379647731781), ('havent', 0.9982263445854187), ('buy', 0.997989296913147), ('mine', 0.9979355335235596), ('sell', 0.997894823551178), ('ok', 0.9978894591331482), ('imagine', 0.9978559613227844), ('solution', 0.9978442192077637), ('driver', 0.9978318810462952)]\n",
      "husband - man + woman ≈ [('national', 0.9823528528213501), ('april', 0.979651689529419), ('among', 0.9783909320831299), ('russian', 0.9755259156227112), ('university', 0.9753519296646118)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "texts = dataset.data[:1000]  # Using a subset for faster training\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts,\n",
    "                      min_count=4,\n",
    "                         window=4,\n",
    "                         vector_size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "# Train TF-IDF model\n",
    "corpus = [\" \".join(tokens) for tokens in tokenized_texts]  # Convert tokenized text back to sentences\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "# Function to compute TF-IDF weighted Word2Vec vectors\n",
    "def get_weighted_w2v_vector(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    vector = np.zeros(300)  # Initialize empty vector of same size as Word2Vec embeddings\n",
    "    total_weight = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in w2v_model.wv and word in tfidf_vocab:\n",
    "            tfidf_weight = tfidf_matrix[0, tfidf_vocab[word]]  # Get TF-IDF weight\n",
    "            vector += w2v_model.wv[word] * tfidf_weight  # Weighted vector sum\n",
    "            total_weight += tfidf_weight\n",
    "\n",
    "    if total_weight > 0:\n",
    "        vector /= total_weight  # Normalize\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Compute vectors for all documents\n",
    "doc_vectors = np.array([get_weighted_w2v_vector(text) for text in texts])\n",
    "\n",
    "# Compute similarity between the first document and all others\n",
    "similarities = cosine_similarity([doc_vectors[0]], doc_vectors)[0]\n",
    "\n",
    "# Rank documents by similarity\n",
    "sorted_indices = np.argsort(similarities)[::-1]  # Sort in descending order\n",
    "\n",
    "print(\"Top 5 most similar documents:\")\n",
    "for idx in sorted_indices[1:6]:  # Skip the first one (itself)\n",
    "    print(f\"Document {idx} - Similarity Score: {similarities[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# Find the most similar words to the first document vector\n",
    "similar_words = w2v_model.wv.similar_by_vector(doc_vectors[0], topn=10)  \n",
    "print(\"Most relevant words to the first document:\", similar_words)\n",
    "\n",
    "\n",
    "# Word analogy: husband - man + woman ≈ ?\n",
    "try:\n",
    "    analogy_vector = w2v_model.wv['husband'] - w2v_model.wv['man'] + w2v_model.wv['woman']\n",
    "    analogy_result = w2v_model.wv.similar_by_vector(analogy_vector, topn=5)\n",
    "    print(\"husband - man + woman ≈\", analogy_result)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e} (Some words may be missing from vocabulary)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
